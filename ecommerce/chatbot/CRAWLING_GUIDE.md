# 웹 데이터 수집(크롤링) 문제 해결 가이드: 무신사 FAQ 사례로 배우기

크롤링은 단순히 "코드를 복사해서 돌리는 것"이 아니라, **"사람이 하는 행동을 기계가 따라하게 만드는 과정"**입니다. 처음 보는 웹사이트에서 데이터를 가져와야 할 때, 개발자는 다음과 같은 사고 과정을 거쳐야 합니다.

## 1단계: 관찰 (사람의 눈으로 분석하기)

코드를 짜기 전에 먼저 브라우저를 열고 F12(개발자 도구)를 눌러 웹사이트를 관찰해야 합니다. 이 단계가 전체의 80%를 차지합니다.

### 🔍 무엇을 봐야 할까요?

1. **URL이 변하는가?**
   - 카테고리를 눌렀을 때 주소창의 URL이 바뀌나요? (`?category=001` 처럼)
   - _무신사 사례:_ URL 파라미터(`mainCategory`)가 바뀌지만, 페이지 전체가 새로고침되지는 않고 내용만 샥 바뀝니다. (SPA 방식)

2. **데이터가 언제 나타나는가?**
   - 페이지에 들어가자마자 글자가 보이나요? (정적 페이지)
   - 버튼을 눌러야 글자가 뿅 하고 나타나나요? (동적 페이지)
   - _무신사 사례:_ 질문 목록은 보이지만, **답변은 질문을 클릭해야만 HTML에 생겨납니다.** 이게 가장 중요한 단서입니다.

3. **규칙성 찾기**
   - 목록이 10개씩 페이징되어 있나요? 아니면 스크롤을 내리면 계속 나오나요?
   - _무신사 사례:_ 페이징 없이 한 번에 다 나오거나, 카테고리를 눌러야 리스트가 갈아끼워집니다.

## 2단계: 도구 선정 (어떻게 가져올까?)

관찰 결과에 따라 도구를 선택합니다.

| 상황                                 | 추천 도구                    | 비유                                                                                              |
| :----------------------------------- | :--------------------------- | :------------------------------------------------------------------------------------------------ |
| **URL 불변, 클릭 필요, 로그인 필요** | **Playwright / Selenium**    | **"로봇 팔"**: 실제 브라우저를 띄워서 사람이 클릭하듯 마우스로 누르고 복사해옴. (느리지만 확실함) |
| **URL이 변하고 데이터가 바로 보임**  | **Requests + BeautifulSoup** | **"편지 배달부"**: 주소(URL)로 요청만 보내고 답장(HTML)을 받아옴. (빠르지만 복잡한 동작 불가)     |
| **API가 숨겨져 있음**                | **Requests (API 호출)**      | **"뒷문 따기"**: 화면 껍데기는 버리고 데이터만 주는 서버 주소로 직접 요청. (가장 빠르고 깔끔함)   |

_무신사 사례:_ 답변을 보려면 **"클릭"**이라는 행동이 필수입니다. 따라서 **"로봇 팔(Playwright)"**을 선택했습니다.

## 3단계: 전략 수립 (작전 지도 그리기)

코딩하기 전에 한글로 순서를 적어봅니다.

1. 무신사 FAQ 페이지에 접속한다.
2. 상단 '전체' 탭부터 '회원정보' 탭까지 하나씩 누른다.
3. 각 탭마다 아래나오는 '소분류' 버튼들도 다 눌러본다.
4. 질문 리스트가 나오면, **첫 번째 질문부터 마지막 질문까지 순서대로 클릭**한다. (클릭해야 답변이 나오니까!)
5. 클릭해서 펼쳐진 답변 텍스트를 복사해서 저장한다.
6. 다음 질문을 클릭... 반복한다.

## 4단계: 구현 및 예외 처리 (로봇 훈련시키기)

이제 코드로 옮깁니다. 이때 중요한 건 **"기다려주기"**입니다.
사람은 화면이 뜰 때까지 무의식적으로 기다리지만, 컴퓨터는 0.1초 만에 "없는데?" 하고 에러를 냅니다.

- **Wait:** 클릭하고 나서 데이터가 로딩될 때까지 `sleep(1)`이나 `wait_for_selector`로 기다려줍니다.
- **Error Handling:** 중간에 팝업이 뜨거나 로딩이 실패해도 멈추지 않고 "로그만 남기고 다음으로 넘어가"도록 `try-except`를 씁니다.

---

## 요약: 다음번에 크롤링 할 때 체크리스트

1. **[F12] 개발자 도구 -> Network 탭**을 켜두고 버튼을 눌러본다.
   - 새로운 요청이 생기면? -> API 방식 가능성 (Requests 사용)
   - 아무 변화 없거나 HTML 요소만 바뀐다면? -> 브라우저 자동화 필요 (Playwright 사용)
2. **"클릭"이나 "스크롤"이 필요한가?**
   - 예 -> Playwright/Selenium 사용
   - 아니오 (주소 치면 바로 다 나옴) -> Requests + BeautifulSoup 사용
3. **데이터가 어디 숨어있는가?**
   - 소스 보기(Ctrl+U) 했을 때 데이터가 있으면 -> 정적 수집 (쉬움)
   - 소스 보기엔 없는데 화면엔 있으면 -> 동적 수집 (JavaScript 실행 필요)

이 사고방식만 있으면 어떤 사이트든 뚫을 수 있습니다!
